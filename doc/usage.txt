"fftMPI documentation"_Manual.html

Using the fftMPI library from your program :h3

The test apps in the test dir are examples of how to use fftMPI from
C++, C, Fortran, and Python.  The details are summarized here for
performing 3d FFTs.  Just change "3" to "2" to perform 2d FFTs.  You
can perform both 2d and 3d FFTs from the same app, by including both
the 2d and 3d header files and linking to both the 2d and 3d library
files.

:line

Calling fftMPI from your source code :h4

Any file that makes a call to fftMPI needs to include a header file
that defines the fftMPI API.  These code lines also show how to
allocate the FFT grid for single or double precision FFTs.

C++: 

#include "fft3d.h"
using namespace FFTMPI_NS; :pre

work = (float *) malloc(2*fftsize*sizeof(float));   // single precision
work = (double *) malloc(2*fftsize*sizeof(double)); // double precision :pre

The header file fft3d.h defines a typedef for FFT_SCALAR which is set
to "float" or "double" depending on the precision you build fftMPI
with.  So you can define the work vector to be of type FFT_SCALAR if
you wish.

C:

#include "fft3d_wrap.h" :pre

work = (float *) malloc(2*fftsize*sizeof(float));   // single precision
work = (double *) malloc(2*fftsize*sizeof(double)); // double precision :pre

As with C++, you can define the work vector to be of type FFT_SCALAR.

Fortran:

use iso_c_binding   ! use these lines in any subroutine that calls fftMPI
use fft3d_wrap :pre

real(4), allocatable, target :: work(:)   ! single precision 
real(8), allocatable, target :: work(:)   ! double precision
allocate(work(2*fftsize)) :pre

Python:

import numpy as np
from fftmpi import FFT3dMPI :pre

work = np.zeros(2*fftsize,np.float32)    # single precision
work = np.zeros(2*fftsize,np.float)      # double precision :pre

In all these code examples, the fftsize variable is assumed to be set
to the total number of FFT grid points owned by a particular
processor.  As explained on the "data layout"_layout.html doc page,
the grid data passed to fftMPI from each processor must be a
contiguous set of complex values.  Thus the allocations are for
2*fftsize values, where 2 is for a "real" value, followed by an
"imaginary value".

As a concrete example, assume the global 3d FFT grid is 1024^3, and a
particular processor owns a 100x200x50 sub-brick of the 3d grid, which
could be located anywhere in the global grid.  Then its fftsize =
100*200*50 = 1 million.  For double precision FFTs, it would allocate
a vector of 2 million 64-bit values.  The details for how each
processor orders values from a 3d sub-grid as a 1d vector are
explained on the "data layout"_layout.html doc page.

IMPORTANT NOTE: As explained on the "compile"_compile.html doc page,
it is a compile-time choice to build fftMPI to perform either single
or double precision FFTs.  Your application must allocate its FFT grid
data to match the library precision setting.  You can look at the test
apps to see how they do this in a flexible way so that the app can
choose to perform its FFTs in either single or double precision.

:line

Building your app with fftMPI :h4

The header files listed above for each language

C++: fft3d.h
C: fft3d_wrap.h
Fortran: fft3d_wrap.f90
Python: fftmpi.py :ul

are all in the fftMPI src directory.  When you compile your app, it
must be able to find the appropriate header file.

For C++ and C, the compile and link commands can be something like this:

mpicxx -I/home/me/fftmpi/src -c test3d.cpp
mpicxx -L/home/me/fftmpi/src test3d.o -lfft3dmpi -o test3d :pre

where the -I and -L switches give the path to the fftMPI src dir.

For Fortran, the fft3d_wrap.f90 file needs to be in the directory with
your app files.  So you can copy it there, as in the test dir.  The
compile and link commands can then be something like this:

mpif90 -I/home/me/fftmpi/src -c fft3d_wrap.f90
mpif90 -I/home/me/fftmpi/src -c test3d_f90.f90
mpif90 -L/home/me/fftmpi/src test3d_f90.o fft3d_wrap.o -lfft3dmpi -lstdc++ -o test3d_f90 :pre

where the -I and -L switches give the path to the fftMPI src dir.

For Python, there is no build step.  However your Python script needs
to be able to find the fftmpi.py file at run time; see the next
section.

Note that if using an external 1d FFT library (FFTW or Intel MKL) then
the link lines above also need to include the 1d FFT library.  See the
"buildtest"_buildtest.html doc page for details on how to do this.  To
use the provided KISS FFT library (just a header file), no additional
link arguments are needed.

:line

Running your app with fftMPI :h4

If you build a C++, C, or Fortran app with a static fftMPI library
(libfft3dmpi.a file), it should just run.

If you build with a shared fftMPI library, then the system must be
able to find the fftMPI library at run time.  This can be done in one
of two ways.

(1) You can add the fftMPI src dir to your LD_LIBRARY_PATH environment
variable, e.g.

setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/home/me/fftmpi/src   # csh or tcsh
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/home/me/fftmpi/src   # bash :pre

(2) You can "install" the fftMPI library file in a location your
system can typically find it, such as /usr/local/lib.  See the
"compile"_compile.html doc page for details on installing fftMPI after
you build it.  This typically requires super-user or sudo priveleges.

To run a Python app that uses fftMPI, two things must work.  If either
operation doesn't work, you will get a Python run-time error.

(a) Python finds the src/fftmpi.py file when a statement like this
is executed:

from fftmpi import FFT3dMPI :pre

You can do this by setting the PYTHONPATH environment variable
to include the fftMPI src dir, like this:

setenv PYTHONPATH ${PYTHONPATH}:/home/sjplimp/fftmpi/src   # csh or tcsh
export PYTHONPATH=${PYTHONPATH}:/home/sjplimp/fftmpi/src   # bash :pre

Or from your Python script, you can augment the search path directly:

path_fftmpi = "/home/me/fftmpi/src"
sys.path.append(path_fftmpi)
from fftmpi import FFT3dMPI :pre

(b) Python finds the fftMPI shared library file (libfft3dmpi.so) when
an FFT instance is instantiated, like this:

fft = FFT3dMPI(world,precision) :pre

You do this the same as explained in (1) or (2) above, either by
adding the fftMPI src dir to the LD_LIBRARY_PATH environment variable,
or copying the fftMPI shared library into a directory where Python can
find it.

:line

Running a Python app in parallel :h4

For a Python app to use fftMPI in parallel, it must pass an MPI
communicator to fftMPI.  Such an app also typically makes MPI calls
in the app itself to work in parallel.  To do both of these in Python,
use the "mpi4py"_http://www.mpi4py.org Python package.

To see if mpi4y is already part of your Python type this line:

>>> from mpi4py import MPI

If it works you should be able to run this test.py script
like this: mpirun -np 4 python test.py

from mpi4py import MPI
world = MPI.COMM_WORLD
me = world.rank
nprocs = world.size
print "Me %d Nprocs %d" % (me,nprocs) :pre

and get 4 lines of output "Me N Nprocs 4", where N = 0,1,2,3.

If the import fails, you need to install mpi4py in your Python.
Here are two ways to do it.

If you are using "anaconda"_http://www.anaconda.org for your Python
package management, simply type the following which will download and
install it:

conda install mpi4py :pre

If not, you can download mpi4py from its web site
"http://www.mpi4py.org"_http://www.mpi4py.org, unpack it, and install
it via pip:

pip install mpi4py :pre

Once installed, the test.py example script above should run.

IMPORTANT NOTE: When mpi4py is installed in your Python, it compiles
an MPI library (e.g. inside conda) or uses a pre-existing MPI library
it finds on your system.  This MUST be the same MPI library that
fftMPI is built with and links to.  If they do not match, you will
typically get run-time MPI errors when your app runs.

You can inspect the path to the MPI library that mpi4py uses like this:

% python
>>> import mpi4py
>>> mpi4py.get_config() :pre
